{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11585380,"sourceType":"datasetVersion","datasetId":7264109}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import wandb\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport random\nfrom torch.nn.utils.rnn import pad_sequence\n\n# Log in to W&B\nwandb.login(key='acdc26d2fc17a56e83ea3ae6c10e496128dee648')\n\n# ---------- Model Definitions ----------\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, cell_type='LSTM', dropout=0.2, bidirectional=False):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[cell_type]\n        self.rnn = rnn_cls(embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n        self.cell_type = cell_type\n        self.bidirectional = bidirectional\n\n    def forward(self, src):\n        embedded = self.embedding(src)\n        outputs, hidden = self.rnn(embedded)\n        return hidden\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers, cell_type='LSTM', dropout=0.2, bidirectional=False):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_dim, embed_dim, padding_idx=0)\n        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[cell_type]\n        self.rnn = rnn_cls(embed_dim, hidden_dim, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n        self.fc_out = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n        self.cell_type = cell_type\n        self.bidirectional = bidirectional\n\n    def forward(self, input, hidden):\n        input = input.unsqueeze(1)\n        embedded = self.embedding(input)\n        output, hidden = self.rnn(embedded, hidden)\n        output = self.fc_out(output.squeeze(1))\n        return output, hidden\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, input_dim, output_dim, embed_dim, hidden_dim, enc_layers, dec_layers,\n                 cell_type='LSTM', dropout=0.2, bidirectional=False):\n        super(Seq2Seq, self).__init__()\n        self.encoder = Encoder(input_dim, embed_dim, hidden_dim, enc_layers, cell_type, dropout, bidirectional)\n        self.decoder = Decoder(output_dim, embed_dim, hidden_dim, dec_layers, cell_type, dropout, bidirectional)\n        self.cell_type = cell_type\n\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size, trg_len = trg.size()\n        outputs = torch.zeros(batch_size, trg_len, self.decoder.fc_out.out_features, device=src.device)\n    \n        hidden = self.encoder(src)\n\n        def flatten_bidirectional(hidden_state):\n            # Convert (num_layers * 2, batch, hidden_size) -> (num_layers, batch, hidden_size * 2)\n            num_layers = self.decoder.rnn.num_layers\n            batch = hidden_state.size(1)\n            hidden_size = hidden_state.size(2)\n            return torch.cat(\n                [hidden_state[i * 2:(i + 1) * 2] for i in range(num_layers)],\n                dim=2\n            )\n    \n        if self.cell_type == 'LSTM':\n            h, c = hidden\n            if self.encoder.bidirectional:\n                h = flatten_bidirectional(h)\n                c = flatten_bidirectional(c)\n            else:\n                h = h[:self.decoder.rnn.num_layers]\n                c = c[:self.decoder.rnn.num_layers]\n            decoder_hidden = (h, c)\n        else:\n            h = hidden\n            if self.encoder.bidirectional:\n                h = flatten_bidirectional(h)\n            else:\n                h = h[:self.decoder.rnn.num_layers]\n            decoder_hidden = h\n    \n        input = trg[:, 0]\n        for t in range(1, trg_len):\n            output, decoder_hidden = self.decoder(input, decoder_hidden)\n            outputs[:, t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.argmax(1)\n            input = trg[:, t] if teacher_force else top1\n    \n        return outputs\n\n\n# ---------- Utility Functions ----------\ndef build_vocab(sequences):\n    chars = set(ch for seq in sequences for ch in seq)\n    stoi = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}\n    for ch in sorted(chars):\n        stoi[ch] = len(stoi)\n    itos = {i: ch for ch, i in stoi.items()}\n    return stoi, itos\n\ndef encode_sequence(seq, stoi):\n    return [stoi.get(c, stoi['<unk>']) for c in seq]\n\ndef prepare_batch(pairs, inp_stoi, out_stoi, device):\n    src_seq = [torch.tensor(encode_sequence(src, inp_stoi) + [inp_stoi['<eos>']]) for src, _ in pairs]\n    trg_seq = [torch.tensor([out_stoi['<sos>']] + encode_sequence(trg, out_stoi) + [out_stoi['<eos>']]) for _, trg in pairs]\n    src_batch = pad_sequence(src_seq, batch_first=True, padding_value=inp_stoi['<pad>'])\n    trg_batch = pad_sequence(trg_seq, batch_first=True, padding_value=out_stoi['<pad>'])\n    return src_batch.to(device), trg_batch.to(device)\n\ndef read_dataset(path):\n    with open(path, encoding='utf-8') as f:\n        lines = f.read().strip().split('\\n')\n        return [(l.split('\\t')[1], l.split('\\t')[0]) for l in lines if '\\t' in l]\n\ndef calculate_accuracy(preds, targets, ignore_index=0):\n    preds = preds.argmax(dim=-1)\n    mask = targets != ignore_index\n    correct = (preds == targets) & mask\n    return (correct.sum().item() / mask.sum().item())*100\n\ndef evaluate(model, data, src_vocab, tgt_vocab, device, criterion, batch_size):\n    model.eval()\n    total_loss = 0\n    total_acc = 0\n    with torch.no_grad():\n        for i in range(0, len(data), batch_size):\n            batch = data[i:i + batch_size]\n            src, trg = prepare_batch(batch, src_vocab, tgt_vocab, device)\n            output = model(src, trg)\n            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n            acc = calculate_accuracy(output[:, 1:], trg[:, 1:])\n            total_loss += loss.item()\n            total_acc += acc\n    return total_loss / len(data), total_acc / (len(data) // batch_size)\n\n# ---------- Test Function ----------\ndef predict_and_log_test_examples(model, test_path, src_vocab, tgt_vocab, device, num_examples=10):\n    model.eval()\n    itos = tgt_vocab[1]  # index to string\n    stoi = src_vocab[0]  # string to index\n\n    test_data = read_dataset(test_path)\n    examples = random.sample(test_data, num_examples)\n    predictions_log = []\n\n    for src_text, tgt_text in examples:\n        src_tensor = torch.tensor(encode_sequence(src_text, stoi) + [stoi['<eos>']], device=device).unsqueeze(0)\n        trg_indexes = [tgt_vocab[0]['<sos>']]\n        hidden = model.encoder(src_tensor)\n\n        if model.cell_type == 'LSTM':\n            decoder_hidden = (hidden[0][:model.decoder.rnn.num_layers], hidden[1][:model.decoder.rnn.num_layers])\n        else:\n            decoder_hidden = hidden[:model.decoder.rnn.num_layers]\n\n        input = torch.tensor([tgt_vocab[0]['<sos>']], device=device)\n\n        decoded_tokens = []\n        for _ in range(30):  # max length\n            output, decoder_hidden = model.decoder(input, decoder_hidden)\n            top1 = output.argmax(1)\n            if top1.item() == tgt_vocab[0]['<eos>']:\n                break\n            decoded_tokens.append(itos[top1.item()])\n            input = top1\n\n        prediction = ''.join(decoded_tokens)\n\n        # Print to notebook output\n        print(f\"Input: {src_text} | Target: {tgt_text} | Prediction: {prediction}\")\n        predictions_log.append(wandb.Html(f\"<b>Input:</b> {src_text} &nbsp; <b>Target:</b> {tgt_text} &nbsp; <b>Pred:</b> {prediction}\"))\n\n    # Log to W&B as a table or HTML\n    wandb.log({\"Test Predictions\": wandb.Html(\"<br>\".join([str(p) for p in predictions_log]))})\n\n\n\n# ---------- Train Function ----------\n\n\ndef train():\n    wandb.init(config={\n        \"embed_dim\": 128,\n        \"hidden_dim\": 256,\n        \"enc_layers\": 2,\n        \"dec_layers\": 2,\n        \"cell_type\": \"LSTM\",\n        \"dropout\": 0.2,\n        \"epochs\": 10,\n        \"batch_size\": 64,\n        \"bidirectional\": False,\n        \"learning_rate\": 0.001,\n        \"optimizer\": \"adam\",\n        \"teacher_forcing_ratio\": 0.5,\n        \"beam_width\": 1\n    })\n    config = wandb.config\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    train_data = read_dataset(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv\")\n    dev_data = read_dataset(\"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv\")\n\n    src_vocab, tgt_vocab = build_vocab([src for src, _ in train_data]), build_vocab([tgt for _, tgt in train_data])\n    model = Seq2Seq(len(src_vocab[0]), len(tgt_vocab[0]), config.embed_dim, config.hidden_dim,\n                    config.enc_layers, config.dec_layers, config.cell_type, config.dropout, config.bidirectional).to(device)\n\n    if config.optimizer == \"adam\":\n        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)\n    elif config.optimizer == \"nadam\":\n        optimizer = optim.NAdam(model.parameters(), lr=config.learning_rate)\n    else:\n        raise ValueError(\"Unsupported optimizer\")\n\n    criterion = nn.CrossEntropyLoss(ignore_index=0)\n\n    for epoch in range(config.epochs):\n        model.train()\n        total_loss = 0\n        total_acc = 0\n        random.shuffle(train_data)\n\n        for i in range(0, len(train_data), config.batch_size):\n            batch = train_data[i:i + config.batch_size]\n            src, trg = prepare_batch(batch, src_vocab[0], tgt_vocab[0], device)\n\n            optimizer.zero_grad()\n            output = model(src, trg, teacher_forcing_ratio=config.teacher_forcing_ratio)\n            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))\n            acc = calculate_accuracy(output[:, 1:], trg[:, 1:])\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n            total_acc += acc\n\n        avg_train_loss = total_loss / len(train_data)\n        avg_train_acc = total_acc / (len(train_data) // config.batch_size)\n        val_loss, val_acc = evaluate(model, dev_data, src_vocab[0], tgt_vocab[0], device, criterion, config.batch_size)\n\n        wandb.log({\n            \"Train Loss\": avg_train_loss,\n            \"Train Accuracy\": avg_train_acc,\n            \"Validation Loss\": val_loss,\n            \"Validation Accuracy\": val_acc,\n            \"Epoch\": epoch + 1,\n            \"Learning Rate\": config.learning_rate,\n            \"Teacher Forcing Ratio\": config.teacher_forcing_ratio,\n            \"Optimizer\": config.optimizer,\n            \"Bidirectional\": config.bidirectional,\n            \"Beam Width\": config.beam_width\n        })\n\n        print(f\"Epoch {epoch + 1}/{config.epochs} | Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n        # At the end of train()\n    \n    test_path = \"/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv\"\n    predict_and_log_test_examples(model, test_path, src_vocab, tgt_vocab, device)\n    wandb.finish()\n\n# ---------- Sweep Setup ----------\nsweep_config = {\n    'method': 'grid',\n    'metric': {'name': 'Validation Loss', 'goal': 'minimize'},\n    'parameters': {\n        'embed_dim': {'values': [32, 64, 256]},\n        'hidden_dim': {'values': [64, 128]},\n        'enc_layers': {'values': [1, 2]},\n        'dec_layers': {'values': [1, 2]},\n        'cell_type': {'values': ['LSTM', 'GRU']},\n        'dropout': {'values': [0.2, 0.3]},\n        'batch_size': {'value': 32},\n        'epochs': {'value': 10},\n        'bidirectional': {'values': [False]},\n        'learning_rate': {'values': [0.001, 0.002, 0.0001]},\n        'optimizer': {'values': ['adam', 'nadam']},\n        'teacher_forcing_ratio': {'values': [0.2, 0.5, 0.7]},\n        'beam_width': {'values': [1, 3, 5]}\n    }\n}\n\nsweep_id = wandb.sweep(sweep_config, project=\"Vinod_Assignment_3_new\")\nwandb.agent(sweep_id, function=train, count=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T17:58:28.121542Z","iopub.execute_input":"2025-05-15T17:58:28.121780Z","iopub.status.idle":"2025-05-15T18:02:56.042329Z","shell.execute_reply.started":"2025-05-15T17:58:28.121761Z","shell.execute_reply":"2025-05-15T18:02:56.041792Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mviinod9\u001b[0m (\u001b[33mviinod9-iitm\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"Create sweep with ID: lo2q1yvj\nSweep URL: https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new/sweeps/lo2q1yvj\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: kt3gkud0 with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbeam_width: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbidirectional: False\n\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdec_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n\u001b[34m\u001b[1mwandb\u001b[0m: \tembed_dim: 32\n\u001b[34m\u001b[1mwandb\u001b[0m: \tenc_layers: 1\n\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_dim: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: adam\n\u001b[34m\u001b[1mwandb\u001b[0m: \tteacher_forcing_ratio: 0.2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.9"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250515_175847-kt3gkud0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new/runs/kt3gkud0' target=\"_blank\">olive-sweep-1</a></strong> to <a href='https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new/sweeps/lo2q1yvj' target=\"_blank\">https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new/sweeps/lo2q1yvj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new' target=\"_blank\">https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new/sweeps/lo2q1yvj' target=\"_blank\">https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new/sweeps/lo2q1yvj</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new/runs/kt3gkud0' target=\"_blank\">https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new/runs/kt3gkud0</a>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10 | Train Loss: 0.0862, Train Acc: 27.57% | Val Loss: 0.0680, Val Acc: 39.72%\nEpoch 2/10 | Train Loss: 0.0592, Train Acc: 44.71% | Val Loss: 0.0496, Val Acc: 53.19%\nEpoch 3/10 | Train Loss: 0.0482, Train Acc: 52.75% | Val Loss: 0.0431, Val Acc: 58.62%\nEpoch 4/10 | Train Loss: 0.0430, Train Acc: 57.09% | Val Loss: 0.0393, Val Acc: 62.09%\nEpoch 5/10 | Train Loss: 0.0398, Train Acc: 60.10% | Val Loss: 0.0373, Val Acc: 64.15%\nEpoch 6/10 | Train Loss: 0.0375, Train Acc: 62.11% | Val Loss: 0.0358, Val Acc: 65.26%\nEpoch 7/10 | Train Loss: 0.0358, Train Acc: 63.87% | Val Loss: 0.0342, Val Acc: 66.93%\nEpoch 8/10 | Train Loss: 0.0345, Train Acc: 65.12% | Val Loss: 0.0332, Val Acc: 67.62%\nEpoch 9/10 | Train Loss: 0.0333, Train Acc: 66.25% | Val Loss: 0.0327, Val Acc: 68.44%\nEpoch 10/10 | Train Loss: 0.0325, Train Acc: 66.99% | Val Loss: 0.0321, Val Acc: 68.70%\nInput: pakaude | Target: पकौड़े | Prediction: पौकाड़\nInput: kabaddi | Target: कबड्डी | Prediction: कबबददीी\nInput: neelaabh | Target: नीलाभ | Prediction: निलबब\nInput: agashe | Target: आगाशे | Prediction: आगेे\nInput: johnson | Target: जॉनसन | Prediction: जोशोंों\nInput: sahakalakar | Target: सहकलाकार | Prediction: सौखाकार\nInput: usmen | Target: उसमें | Prediction: उस्मीन\nInput: afroz | Target: अफरोज | Prediction: अफरोवा\nInput: race | Target: रेस | Prediction: रेस\nInput: chhapemar | Target: छापेमार | Prediction: छापममार\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Beam Width</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>Learning Rate</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Teacher Forcing Ratio</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Accuracy</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▄▆▆▇▇████</td></tr><tr><td>Validation Loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Beam Width</td><td>1</td></tr><tr><td>Bidirectional</td><td>False</td></tr><tr><td>Epoch</td><td>10</td></tr><tr><td>Learning Rate</td><td>0.001</td></tr><tr><td>Optimizer</td><td>adam</td></tr><tr><td>Teacher Forcing Ratio</td><td>0.2</td></tr><tr><td>Train Accuracy</td><td>66.99424</td></tr><tr><td>Train Loss</td><td>0.03252</td></tr><tr><td>Validation Accuracy</td><td>68.69594</td></tr><tr><td>Validation Loss</td><td>0.03208</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">olive-sweep-1</strong> at: <a href='https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new/runs/kt3gkud0' target=\"_blank\">https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new/runs/kt3gkud0</a><br> View project at: <a href='https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new' target=\"_blank\">https://wandb.ai/viinod9-iitm/Vinod_Assignment_3_new</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250515_175847-kt3gkud0/logs</code>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}