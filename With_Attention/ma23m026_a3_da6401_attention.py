# -*- coding: utf-8 -*-
"""ma23m026-a3-da6401-attention

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/viinod9/ma23m026-a3-da6401-attention.02d71957-2302-4f56-8f5a-ac2bcc1a4c95.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250519/auto/storage/goog4_request%26X-Goog-Date%3D20250519T162300Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9c0cda90f76ca9f987c3fb1efa4216211730bae5a831eace3a5a9a0aeb8fe247e36e4a254f14c77d97666dee660a3a90af67642290a57a55521062c7c19fc50b3be5d534b73a9eeb66151b418339b5386ac66e690923409a81e736b6f81a9baa9821969e41c864c7df7eb3007ea41897bda61dd8e97e9fb65b59ba641fbf7608acbb38dce97fe12b3c5e3b2835df3f956670e93cf1c2f3777bdd4224ed62f4a529cc540bd7a4b1a93c06cafaa3871dfde6a63607342c850959d9ee770bdc69ec083e82fc50dbc8db6716e8cc4a1a822b80f1fe984db757f119e4cec9208fda4361c8740ae612393d75272be9318771aa1431c252302f5bd62f1cab4a5788a325
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.
import kagglehub
viinod9_dakshina_dataset_path = kagglehub.dataset_download('viinod9/dakshina-dataset')
viinod9_nato_sans_devnagari_path = kagglehub.dataset_download('viinod9/nato-sans-devnagari')

print('Data source import complete.')


# Import necessary libraries
import wandb  # for logging visualizations and metrics
import torch
import torch.nn as nn
import torch.optim as optim
import random
from torch.nn.utils.rnn import pad_sequence

# Define the Attention mechanism used in the decoder
class Attention(nn.Module):
    def __init__(self, enc_hidden_dim, dec_hidden_dim):
        super().__init__()
        # Linear layer to compute attention energy
        self.attn = nn.Linear(enc_hidden_dim + dec_hidden_dim, dec_hidden_dim)
        # Linear layer to produce scalar attention scores
        self.v = nn.Linear(dec_hidden_dim, 1, bias=False)

    def forward(self, hidden, encoder_outputs, mask=None):
        # Repeat the decoder hidden state across the time dimension
        hidden = hidden.unsqueeze(1).repeat(1, encoder_outputs.size(1), 1)
        # Concatenate decoder hidden state and encoder outputs, then apply non-linearity
        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))
        # Apply linear layer and squeeze to get attention scores
        attention = self.v(energy).squeeze(2)
        # Mask attention scores if padding mask is provided
        if mask is not None:
            attention = attention.masked_fill(mask == 0, -1e10)
        # Return normalized attention weights
        return torch.softmax(attention, dim=1)

# Define the Encoder module
class Encoder(nn.Module):
    def __init__(self, input_dim, embed_dim, hidden_dim, cell_type='LSTM', dropout=0.2):
        super().__init__()
        # Embedding layer with padding_idx=0
        self.embedding = nn.Embedding(input_dim, embed_dim, padding_idx=0)
        # Select the RNN variant (RNN, LSTM, GRU)
        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[cell_type]
        # Define the RNN layer
        self.rnn = rnn_cls(embed_dim, hidden_dim, 1, dropout=dropout, batch_first=True)
        self.cell_type = cell_type

    def forward(self, src):
        # Embed the input sequence
        embedded = self.embedding(src)
        # Pass embeddings through the RNN
        outputs, hidden = self.rnn(embedded)
        # Return all outputs and final hidden state
        return outputs, hidden

# Define the Attention-based Decoder module
class AttnDecoder(nn.Module):
    def __init__(self, output_dim, embed_dim, hidden_dim, enc_hidden_dim, attention, cell_type='LSTM', dropout=0.2):
        super().__init__()
        # Embedding layer for decoder input
        self.embedding = nn.Embedding(output_dim, embed_dim, padding_idx=0)
        self.attention = attention
        # Select the RNN variant
        rnn_cls = {'RNN': nn.RNN, 'LSTM': nn.LSTM, 'GRU': nn.GRU}[cell_type]
        # RNN takes concatenated [embedded, context] as input
        self.rnn = rnn_cls(embed_dim + enc_hidden_dim, hidden_dim, 1, dropout=dropout, batch_first=True)
        # Final output layer to map RNN output to vocabulary
        self.fc_out = nn.Linear(hidden_dim, output_dim)
        self.cell_type = cell_type

    def forward(self, input, hidden, encoder_outputs):
        input = input.unsqueeze(1)  # Make input shape [batch_size, 1]
        embedded = self.embedding(input)  # Embed the input token

        # Select the hidden state to use for attention
        if self.cell_type == 'LSTM':
            dec_hidden = hidden[0][-1]  # Take last layer of hidden state (h_n)
        else:
            dec_hidden = hidden[-1]

        # Calculate attention weights
        attn_weights = self.attention(dec_hidden, encoder_outputs)
        # Compute context vector as weighted sum of encoder outputs
        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs)
        # Concatenate context and embedded input
        rnn_input = torch.cat((embedded, context), dim=2)
        # Pass through RNN
        output, hidden = self.rnn(rnn_input, hidden)
        # Generate output token probabilities
        output = self.fc_out(output.squeeze(1))
        return output, hidden, attn_weights

# Define the Seq2Seq model that combines Encoder and Attention-based Decoder
class Seq2Seq(nn.Module):
    def __init__(self, input_dim, output_dim, embed_dim, hidden_dim, cell_type='LSTM', dropout=0.2):
        super().__init__()
        # Instantiate encoder
        self.encoder = Encoder(input_dim, embed_dim, hidden_dim, cell_type, dropout)
        # Instantiate attention mechanism
        self.attention = Attention(hidden_dim, hidden_dim)
        # Instantiate decoder
        self.decoder = AttnDecoder(output_dim, embed_dim, hidden_dim, hidden_dim, self.attention, cell_type, dropout)
        self.cell_type = cell_type

    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        batch_size, trg_len = trg.size()
        # Initialize output and attention tensor
        outputs = torch.zeros(batch_size, trg_len, self.decoder.fc_out.out_features, device=src.device)
        attentions = torch.zeros(batch_size, trg_len, src.size(1), device=src.device)

        # Encode source sequence
        encoder_outputs, hidden = self.encoder(src)

        input = trg[:, 0]  # Start with <sos> token
        for t in range(1, trg_len):
            # Decode next token
            output, hidden, attn_weights = self.decoder(input, hidden, encoder_outputs)
            outputs[:, t] = output
            attentions[:, t] = attn_weights
            # Apply teacher forcing
            teacher_force = random.random() < teacher_forcing_ratio
            top1 = output.argmax(1)
            input = trg[:, t] if teacher_force else top1

        return outputs, attentions


########## Helper Function for Logging Attention Heatmaps

import matplotlib.pyplot as plt
from matplotlib import font_manager

def log_attention_heatmaps_with_hindi(model, src_vocab, tgt_vocab, device, test_data, num_examples=9, font_path="/kaggle/input/nato-sans-devnagari/static/NotoSansDevanagari-Regular.ttf"):
    model.eval()  # Set model to evaluation mode
    examples = random.sample(test_data, num_examples)  # Randomly sample examples

    # Load Hindi font for Devanagari script rendering
    hindi_font = font_manager.FontProperties(fname=font_path)

    # Set up subplot grid for displaying attention maps
    fig, axes = plt.subplots(3, 3, figsize=(15, 15))
    axes = axes.flatten()

    for idx, (src_text, tgt_text) in enumerate(examples):
        # Encode source and target texts to ID sequences
        src_ids = encode_sequence(src_text, src_vocab[0]) + [src_vocab[0]['<eos>']]
        trg_ids = [tgt_vocab[0]['<sos>']] + encode_sequence(tgt_text, tgt_vocab[0]) + [tgt_vocab[0]['<eos>']]

        # Convert sequences to tensors
        src_tensor = torch.tensor(src_ids, device=device).unsqueeze(0)
        trg_tensor = torch.tensor(trg_ids, device=device).unsqueeze(0)

        # Run forward pass without teacher forcing
        outputs, attentions = model(src_tensor, trg_tensor, teacher_forcing_ratio=0)

        # Extract attention weights for each decoder step
        attn = attentions[0, 1:len(trg_tensor[0])-1].detach().cpu().numpy()

        ax = axes[idx]
        # Plot attention heatmap
        im = ax.imshow(attn, aspect='auto', cmap='viridis')

        # Convert token IDs to strings
        src_tokens = [src_vocab[1][i] for i in src_ids]
        trg_tokens = [tgt_vocab[1][i] for i in trg_ids[1:-1]]

        # Set axis ticks with Hindi font labels
        ax.set_xticks(range(len(src_tokens)))
        ax.set_xticklabels(src_tokens, fontsize=12, rotation=90, fontproperties=hindi_font)

        ax.set_yticks(range(len(trg_tokens)))
        ax.set_yticklabels(trg_tokens, fontsize=12, fontproperties=hindi_font)

        ax.set_title(f"Input: {src_text}\nTarget: {tgt_text}", fontsize=14, fontproperties=hindi_font)

    plt.tight_layout()
    # Log attention heatmaps to wandb
    wandb.log({"Attention Heatmaps Hindi": wandb.Image(fig)})
    plt.show()
    plt.close()

import pandas as pd

# Log in to Weights & Biases for experiment tracking
wandb.login(key='acdc26d2fc17a56e83ea3ae6c10e496128dee648')

# ---------- Utility Functions ----------

# Build vocabulary from character-level sequences
def build_vocab(sequences):
    chars = set(ch for seq in sequences for ch in seq)  # Collect all unique characters
    stoi = {'<pad>': 0, '<sos>': 1, '<eos>': 2, '<unk>': 3}  # Special tokens
    for ch in sorted(chars):
        stoi[ch] = len(stoi)  # Assign index to each character
    itos = {i: ch for ch, i in stoi.items()}  # Inverse mapping
    return stoi, itos

# Convert a sequence of characters to a list of token ids using stoi mapping
def encode_sequence(seq, stoi):
    return [stoi.get(c, stoi['<unk>']) for c in seq]  # Unknown characters get '<unk>' index

# Prepare input-output batches from sequence pairs
def prepare_batch(pairs, inp_stoi, out_stoi, device):
    # Encode and append <eos> to source; add <sos> and <eos> to target
    src_seq = [torch.tensor(encode_sequence(src, inp_stoi) + [inp_stoi['<eos>']]) for src, _ in pairs]
    trg_seq = [torch.tensor([out_stoi['<sos>']] + encode_sequence(trg, out_stoi) + [out_stoi['<eos>']]) for _, trg in pairs]
    # Pad sequences in batch
    src_batch = pad_sequence(src_seq, batch_first=True, padding_value=inp_stoi['<pad>'])
    trg_batch = pad_sequence(trg_seq, batch_first=True, padding_value=out_stoi['<pad>'])
    return src_batch.to(device), trg_batch.to(device)

# Read parallel dataset file (each line has <target>\t<input>)
def read_dataset(path):
    with open(path, encoding='utf-8') as f:
        lines = f.read().strip().split('\n')
        return [(l.split('\t')[1], l.split('\t')[0]) for l in lines if '\t' in l]

# Compute word-level accuracy by comparing full predicted vs. target sequences
def calculate_word_accuracy(preds, targets, ignore_index=0):
    preds = preds.argmax(dim=-1)  # Get token predictions by choosing max probability
    mask = targets != ignore_index  # Mask out padded tokens

    # Apply mask and compare predictions to targets
    preds_masked = preds * mask
    targets_masked = targets * mask

    # Check whether each entire sequence matches
    sequence_correct = (preds_masked == targets_masked).all(dim=1)

    # Compute percentage of correctly predicted full sequences
    word_accuracy = sequence_correct.float().mean().item() * 100
    return word_accuracy

# Alternative version: directly use predicted and target token ID tensors
def calculate_word_accuracy_from_ids(preds_ids, targets_ids, ignore_index=0):
    mask = targets_ids != ignore_index  # Ignore padded tokens
    print(mask)  # Debug print

    preds_masked = preds_ids * mask
    targets_masked = targets_ids * mask

    sequence_correct = (preds_masked == targets_masked).all(dim=1)
    word_accuracy = sequence_correct.float().mean().item() * 100
    return word_accuracy

# Evaluate model on a dataset and compute average loss and accuracy
def evaluate(model, data, src_vocab, tgt_vocab, device, criterion, batch_size):
    model.eval()  # Set model to evaluation mode
    total_loss = 0
    total_acc = 0
    with torch.no_grad():  # Disable gradient computation
        for i in range(0, len(data), batch_size):
            batch = data[i:i + batch_size]
            src, trg = prepare_batch(batch, src_vocab, tgt_vocab, device)
            output, _ = model(src, trg)  # Forward pass
            # Ignore first token (<sos>) during loss calculation
            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))
            acc = calculate_word_accuracy(output[:, 1:], trg[:, 1:])
            total_loss += loss.item()
            total_acc += acc
    return total_loss / len(data), total_acc / (len(data) // batch_size)

# Predict on test examples and log predictions and attention using wandb
def predict_and_log_test_examples_with_attention(model, test_path, src_vocab, tgt_vocab, device, num_examples=50, output_csv_path="predictions_attention.csv"):
    model.eval()
    itos = tgt_vocab[1]  # Index-to-character for target vocabulary
    stoi = src_vocab[0]  # Character-to-index for source vocabulary

    test_data = read_dataset(test_path)
    examples = random.sample(test_data, num_examples)  # Sample test examples
    predictions_log = []

    preds_list = []
    trgs_list = []
    csv_records = []  # Records to save to CSV

    for src_text, tgt_text in examples:
        # Encode source and target sequences
        src_tensor = torch.tensor(encode_sequence(src_text, stoi) + [stoi['<eos>']], device=device).unsqueeze(0)
        trg_tensor = torch.tensor([tgt_vocab[0]['<sos>']] + encode_sequence(tgt_text, tgt_vocab[0]) + [tgt_vocab[0]['<eos>']], device=device).unsqueeze(0)

        # Run encoder
        encoder_outputs, hidden = model.encoder(src_tensor)

        # Decoder starts with <sos> token
        input = torch.tensor([tgt_vocab[0]['<sos>']], device=device)
        decoder_hidden = hidden

        decoded_tokens = []
        all_attn_weights = []  # For attention visualization

        for _ in range(30):  # Max decoding steps
            output, decoder_hidden, attn_weights = model.decoder(input, decoder_hidden, encoder_outputs)
            top1 = output.argmax(1)  # Select top prediction
            if top1.item() == tgt_vocab[0]['<eos>']:
                break  # Stop decoding at <eos>
            decoded_tokens.append(top1.item())
            all_attn_weights.append(attn_weights.detach().cpu().numpy())  # Store attention weights
            input = top1  # Feed next input

        prediction = decoded_tokens
        pred_str = ''.join([itos[idx] for idx in prediction])  # Convert prediction to string

        # Save example to CSV
        csv_records.append({
            "Input": src_text,
            "Target": tgt_text,
            "Prediction": pred_str
        })

        # Print and log prediction using wandb
        print(f"Input: {src_text} | Target: {tgt_text} | Prediction: {pred_str}")
        predictions_log.append(wandb.Html(f"<b>Input:</b> {src_text} &nbsp; <b>Target:</b> {tgt_text} &nbsp; <b>Pred:</b> {pred_str}"))

        # Prepare for accuracy computation
        tgt_encoded = [tgt_vocab[0].get(ch, tgt_vocab[0]['<unk>']) for ch in tgt_text] + [tgt_vocab[0]['<eos>']]
        preds_list.append(torch.tensor(prediction, device=device))
        trgs_list.append(torch.tensor(tgt_encoded, device=device))

    # Pad sequences to same length for accuracy computation
    max_len = max(max([p.size(0) for p in preds_list]), max([t.size(0) for t in trgs_list]))
    preds_padded = pad_sequence([torch.cat([p, torch.full((max_len - p.size(0),), 0, device=device)]) if p.size(0) < max_len else p for p in preds_list], batch_first=True)
    trgs_padded = pad_sequence([torch.cat([t, torch.full((max_len - t.size(0),), 0, device=device)]) if t.size(0) < max_len else t for t in trgs_list], batch_first=True)

    # Calculate word accuracy on test examples
    test_word_acc = calculate_word_accuracy_from_ids(preds_padded, trgs_padded)
    print(f"Test Word Accuracy on {num_examples} examples: {test_word_acc:.2f}%")

    # Log predictions and accuracy to wandb
    wandb.log({
        "Test Predictions": wandb.Html("<br>".join([str(p) for p in predictions_log])),
        "Test Word Accuracy": test_word_acc
    })

    # Save predictions to CSV file
    df = pd.DataFrame(csv_records)
    df.to_csv(output_csv_path, index=False)
    print(f"Predictions saved to {output_csv_path}")
    return test_word_acc

# ---------- Train Function ----------
def train():
    # Initialize a Weights & Biases (wandb) run with configuration settings
    wandb.init(config={
        "embed_dim": 128,                   # Embedding dimension for source/target tokens
        "hidden_dim": 256,                  # Hidden layer size for RNN cell
        "cell_type": "LSTM",                # Type of RNN cell to use (LSTM/GRU/RNN)
        "dropout": 0.2,                     # Dropout rate for regularization
        "epochs": 10,                       # Number of training epochs
        "batch_size": 64,                   # Mini-batch size
        "learning_rate": 0.001,             # Learning rate for optimizer
        "optimizer": "adam",                # Optimizer to use ('adam' or 'nadam')
        "teacher_forcing_ratio": 0.5,       # Probability of using teacher forcing during training
    })

    # Get configuration from wandb
    config = wandb.config

    # Set device to GPU if available, else CPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Load training and validation datasets (Dakshina Hindi transliteration)
    train_data = read_dataset("/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv")
    dev_data = read_dataset("/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv")

    # Build source and target vocabularies from training data
    src_vocab, tgt_vocab = build_vocab([src for src, _ in train_data]), build_vocab([tgt for _, tgt in train_data])

    # Initialize Seq2Seq model with attention (1-layer encoder-decoder as per assignment requirement)
    model = Seq2Seq(len(src_vocab[0]), len(tgt_vocab[0]), config.embed_dim, config.hidden_dim,
                    config.cell_type, config.dropout).to(device)

    # Initialize optimizer based on configuration
    if config.optimizer == "adam":
        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)
    elif config.optimizer == "nadam":
        optimizer = optim.NAdam(model.parameters(), lr=config.learning_rate)
    else:
        raise ValueError("Unsupported optimizer")  # Raise error if unsupported optimizer is given

    # Define loss function (ignoring padding index = 0)
    criterion = nn.CrossEntropyLoss(ignore_index=0)

    # Loop over all epochs
    for epoch in range(config.epochs):
        model.train()                       # Set model to training mode
        total_loss = 0                      # Track cumulative training loss
        total_acc = 0                       # Track cumulative word-level accuracy
        random.shuffle(train_data)         # Shuffle training data before each epoch

        # Process data in mini-batches
        for i in range(0, len(train_data), config.batch_size):
            batch = train_data[i:i + config.batch_size]

            # Prepare input and target tensors for the batch
            src, trg = prepare_batch(batch, src_vocab[0], tgt_vocab[0], device)

            optimizer.zero_grad()          # Reset gradients
            output, _ = model(src, trg, teacher_forcing_ratio=config.teacher_forcing_ratio)

            # Compute loss (excluding the <sos> token in output and target)
            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))

            # Calculate word-level accuracy for the batch
            acc = calculate_word_accuracy(output[:, 1:], trg[:, 1:])

            loss.backward()                # Backpropagate loss
            optimizer.step()               # Update model parameters

            total_loss += loss.item()      # Accumulate loss
            total_acc += acc               # Accumulate accuracy

        # Calculate average loss and accuracy over all training batches
        avg_train_loss = total_loss / len(train_data)
        avg_train_acc = total_acc / (len(train_data) // config.batch_size)

        # Evaluate model on validation data
        val_loss, val_acc = evaluate(model, dev_data, src_vocab[0], tgt_vocab[0], device, criterion, config.batch_size)

        # Log metrics to wandb
        wandb.log({
            "Train Loss": avg_train_loss,
            "Train Accuracy": avg_train_acc,
            "Validation Loss": val_loss,
            "Validation Accuracy": val_acc,
            "Epoch": epoch + 1,
            "Learning Rate": config.learning_rate,
            "Teacher Forcing Ratio": config.teacher_forcing_ratio,
        })

        # Print epoch-wise performance
        print(f"Epoch {epoch + 1}/{config.epochs} | Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")

    # Finish the wandb run
    wandb.finish()


# ---------- Sweep Setup ----------
# Define a sweep configuration for hyperparameter tuning using Bayesian optimization
sweep_config = {
    'method': 'bayes',  # Bayesian optimization for efficient hyperparameter search
    'metric': {'name': 'Validation Accuracy', 'goal': 'maximize'},  # Target metric to optimize

    # Search space for hyperparameters
    'parameters': {
        'embed_dim': {'values': [32, 64, 256]},             # Embedding dimension options
        'hidden_dim': {'values': [64, 128]},                # Hidden layer sizes
        'enc_layers': {'values': [1,2,3]},                  # Number of encoder layers
        'dec_layers': {'values': [1,2,3]},                  # Number of decoder layers
        'cell_type': {'values': ['LSTM','GRU','RNN']},      # RNN variant to use
        'dropout': {'values': [0.2, 0.3]},                  # Dropout rate
        'batch_size': {'values': [32,64]},                  # Batch sizes
        'epochs': {'values': [5,10,15]},                    # Epoch counts
        'bidirectional': {'values': [False]},               # (Fixed) Use unidirectional encoder
        'learning_rate': {'values': [0.001, 0.002, 0.0001]},# Learning rates
        'optimizer': {'values': ['adam', 'nadam']},         # Optimizer choices
        'teacher_forcing_ratio': {'values': [0.2, 0.5, 0.7]}, # Teacher forcing ratios
        'beam_width': {'values': [1, 3, 5]}                 # Beam search width for decoding
    }
}

# Create sweep on wandb and start agent to run 80 trials with different hyperparameter combinations
# The 'train' function is used as the training routine
sweep_id = wandb.sweep(sweep_config, project="with_attention_sweep")
wandb.agent(sweep_id, function=train, count=80)



"""# Test"""

# ---------- Train Function ----------

def train_for_test():
    wandb.init(config={
        "embed_dim": 32,
        "hidden_dim": 128,
        "cell_type": "LSTM",
        "dropout": 0.2,
        "epochs": 15,
        "batch_size": 32,
        "learning_rate": 0.001,
        "optimizer": "adam",
        "teacher_forcing_ratio": 0.7,
    })


    config = wandb.config
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Load data
    train_data = read_dataset("/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.train.tsv")
    dev_data = read_dataset("/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.dev.tsv")

    # Build vocabs
    src_vocab, tgt_vocab = build_vocab([src for src, _ in train_data]), build_vocab([tgt for _, tgt in train_data])

    # Attention Seq2Seq model with 1 layer encoder-decoder (as per assignment requirement)
    model = Seq2Seq(len(src_vocab[0]), len(tgt_vocab[0]), config.embed_dim, config.hidden_dim,
                    config.cell_type, config.dropout).to(device)

    # Optimizer
    if config.optimizer == "adam":
        optimizer = optim.Adam(model.parameters(), lr=config.learning_rate)
    elif config.optimizer == "nadam":
        optimizer = optim.NAdam(model.parameters(), lr=config.learning_rate)
    else:
        raise ValueError("Unsupported optimizer")

    criterion = nn.CrossEntropyLoss(ignore_index=0)

    for epoch in range(config.epochs):
        model.train()
        total_loss = 0
        total_acc = 0
        random.shuffle(train_data)

        for i in range(0, len(train_data), config.batch_size):
            batch = train_data[i:i + config.batch_size]
            src, trg = prepare_batch(batch, src_vocab[0], tgt_vocab[0], device)

            optimizer.zero_grad()
            output, _ = model(src, trg, teacher_forcing_ratio=config.teacher_forcing_ratio)
            loss = criterion(output[:, 1:].reshape(-1, output.shape[-1]), trg[:, 1:].reshape(-1))
            acc = calculate_word_accuracy(output[:, 1:], trg[:, 1:])
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            total_acc += acc

        avg_train_loss = total_loss / len(train_data)
        avg_train_acc = total_acc / (len(train_data) // config.batch_size)
        val_loss, val_acc = evaluate(model, dev_data, src_vocab[0], tgt_vocab[0], device, criterion, config.batch_size)

        wandb.log({
            "Train Loss": avg_train_loss,
            "Train Accuracy": avg_train_acc,
            "Validation Loss": val_loss,
            "Validation Accuracy": val_acc,
            "Epoch": epoch + 1,
            "Learning Rate": config.learning_rate,
            "Teacher Forcing Ratio": config.teacher_forcing_ratio,
        })

        print(f"Epoch {epoch + 1}/{config.epochs} | Train Loss: {avg_train_loss:.4f}, Train Acc: {avg_train_acc:.2f}% | Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")

    # Log attention heatmaps at the end of training
    # Load your test data
    test_data = read_dataset("/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv")

    # Call the function to predict and log test examples + word accuracy
    # predict_and_log_test_examples_with_attention(
    #     model=model,
    #     test_path="/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv",
    #     src_vocab=src_vocab,
    #     tgt_vocab=tgt_vocab,
    #     device=device,
    #     num_examples=len(test_data),  # You can change to 9, 20 etc.
    #     output_csv_path="attention_predictions.csv"  # <-- desired CSV path
    # )


    test_acc = predict_and_log_test_examples_with_attention(
    model=model,
    test_path="/kaggle/input/dakshina-dataset/dakshina_dataset_v1.0/hi/lexicons/hi.translit.sampled.test.tsv",
    src_vocab=src_vocab,
    tgt_vocab=tgt_vocab,
    device=device,
    num_examples=len(test_data),  # You can change to 9, 20 etc.
    output_csv_path="/kaggle/working/attention_predictions_new.csv" )
    # <-- desired CSV path

    # Log test accuracy to wandb
    wandb.log({"Test Accuracy": test_acc})

    #print(f"Test Accuracy: {test_acc:.2f}%")



    log_attention_heatmaps_with_hindi(
            model=model,
            src_vocab=src_vocab,
            tgt_vocab=tgt_vocab,
            device=device,
            test_data=test_data,
            num_examples=9, # Exactly 9 for 3x3 grid
            font_path="/kaggle/input/nato-sans-devnagari/static/NotoSansDevanagari-Regular.ttf"
        )

    wandb.finish()


# ---------- Sweep Setup ----------
sweep_config = {
    'method': 'random',
    'metric': {'name': 'Validation Loss', 'goal': 'minimize'},
    'parameters': {
        'embed_dim': {'values': [32]},
        'hidden_dim': {'values': [128]},
        'enc_layers': {'values': [1]},
        'dec_layers': {'values': [3]},
        'cell_type': {'values': ['LSTM']},
        'dropout': {'values': [0.2]},
        'batch_size': {'value': 32},
        'epochs': {'value': 15},
        'bidirectional': {'values': [False]},
        'learning_rate': {'values': [0.001]},
        'optimizer': {'values': ['adam']},
        'teacher_forcing_ratio': {'values': [0.7]},
        'beam_width': {'values': [1]}
    }
}



# LSTM , GRU, RNN 1 3
sweep_id = wandb.sweep(sweep_config, project="with_attention_sweep_best_model_test_new123")
wandb.agent(sweep_id, function=train_for_test, count=1)



